#!/usr/bin/env python3
"""
æœ€çµ‚ãƒ•ã‚§ãƒ¼ã‚º: 2000å•ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ï¼ˆä¿®æ­£ç‰ˆï¼‰
"""
import re
import csv
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime

def build_complete_database():
    """2000å•ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
    print("ğŸ—ï¸ å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ï¼ˆæœ€çµ‚ãƒ•ã‚§ãƒ¼ã‚ºï¼‰")
    print("=" * 60)
    
    # ãƒ‘ã‚¹è¨­å®š
    exports_dir = Path("/workspaces/jmle-explanation-generator/raw_data/exports")
    notion_dir = Path("/workspaces/jmle-explanation-generator/raw_data/notion")
    final_dir = Path("/workspaces/jmle-explanation-generator/final_database")
    final_dir.mkdir(exist_ok=True)
    
    # 1. çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿")
    print("-" * 40)
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    integrated_csv = exports_dir / "integrated_dataset_2000.csv"
    integrated_data = {}
    
    if integrated_csv.exists():
        with open(integrated_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                qid = row['å•é¡ŒID']
                integrated_data[qid] = row
        print(f"  çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(integrated_data)}å•")
    
    # æ—¢å­˜Notionãƒ‡ãƒ¼ã‚¿
    notion_files = list(notion_dir.glob("*.csv"))
    existing_notion = {}
    
    if notion_files:
        with open(notion_files[0], 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                qid = None
                for key in row.keys():
                    if 'å•é¡ŒID' in key:
                        qid = row[key]
                        break
                
                if qid and qid.strip() and re.match(r'\d{3}[A-Z]\d{1,3}', qid.strip()):
                    existing_notion[qid.strip()] = row
        print(f"  æ—¢å­˜Notionãƒ‡ãƒ¼ã‚¿: {len(existing_notion)}å•")
    
    # å¾©å…ƒãƒ‡ãƒ¼ã‚¿
    restore_csv = exports_dir / "notion_missing_questions_restore.csv"
    restore_data = {}
    
    if restore_csv.exists():
        with open(restore_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                qid = None
                for key in row.keys():
                    if 'å•é¡ŒID' in key:
                        qid = row[key]
                        break
                
                if qid and qid.strip():
                    restore_data[qid.strip()] = row
        print(f"  å¾©å…ƒãƒ‡ãƒ¼ã‚¿: {len(restore_data)}å•")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ
    print("\nğŸ”— ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ")
    print("-" * 40)
    
    complete_database = {}
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¨å•é¡Œã‚’åŸºæº–ã¨ã™ã‚‹
    for qid in sorted(integrated_data.keys()):
        # åŸºæœ¬æƒ…å ±
        integrated_row = integrated_data[qid]
        
        # å®Œå…¨ãªè¨˜éŒ²ä½œæˆ
        complete_record = {
            'å•é¡ŒID': qid,
            'å¹´åº¦': qid[:3],
            'ã‚»ã‚¯ã‚·ãƒ§ãƒ³': qid[3],
            'å•é¡Œç•ªå·': qid[4:],
            'ã‚½ãƒ¼ã‚¹': integrated_row.get('ã‚½ãƒ¼ã‚¹', ''),
            'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„': integrated_row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', ''),
            'ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹': 'complete'
        }
        
        # Notionã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯çµ±åˆ
        if qid in existing_notion:
            notion_row = existing_notion[qid]
            complete_record['æ­£ç­”'] = notion_row.get('æ­£ç­”', '')
            complete_record['æ­£ç­”ç‡'] = notion_row.get('æ­£ç­”ç‡', '')
            complete_record['è‹±èªå•é¡Œ'] = notion_row.get('è‹±èªå•é¡Œ', 'No')
            complete_record['ç”»åƒå•é¡Œ'] = notion_row.get('ç”»åƒå•é¡Œ', 'No')
            complete_record['é€£å•'] = notion_row.get('é€£å•', 'No')
            complete_record['è¨ˆç®—å•é¡Œ'] = notion_row.get('è¨ˆç®—å•é¡Œ', 'No')
            complete_record['å•é¡Œæ–‡'] = notion_row.get('å•é¡Œæ–‡', '')
            complete_record['ç—‡ä¾‹æ–‡'] = notion_row.get('ç—‡ä¾‹æ–‡', '')
            complete_record['é¸æŠè‚¢'] = notion_row.get('é¸æŠè‚¢', '')
            complete_record['Webè¡¨ç¤ºç”¨'] = notion_row.get('Webè¡¨ç¤ºç”¨', '')
            complete_record['ã‚¿ã‚°'] = notion_row.get('ã‚¿ã‚°', '')
            complete_record['æœ€çµ‚æ›´æ–°'] = notion_row.get('æœ€çµ‚æ›´æ–°', '')
            complete_record['ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹'] = 'notion_existing'
        elif qid in restore_data:
            restore_row = restore_data[qid]
            complete_record['æ­£ç­”'] = restore_row.get('æ­£ç­”', '')
            complete_record['æ­£ç­”ç‡'] = restore_row.get('æ­£ç­”ç‡', '')
            complete_record['è‹±èªå•é¡Œ'] = restore_row.get('è‹±èªå•é¡Œ', 'No')
            complete_record['ç”»åƒå•é¡Œ'] = restore_row.get('ç”»åƒå•é¡Œ', 'No')
            complete_record['é€£å•'] = restore_row.get('é€£å•', 'No')
            complete_record['è¨ˆç®—å•é¡Œ'] = restore_row.get('è¨ˆç®—å•é¡Œ', 'No')
            complete_record['å•é¡Œæ–‡'] = restore_row.get('å•é¡Œæ–‡', '')
            complete_record['ç—‡ä¾‹æ–‡'] = restore_row.get('ç—‡ä¾‹æ–‡', '')
            complete_record['é¸æŠè‚¢'] = restore_row.get('é¸æŠè‚¢', '')
            complete_record['Webè¡¨ç¤ºç”¨'] = restore_row.get('Webè¡¨ç¤ºç”¨', complete_record['ã‚³ãƒ³ãƒ†ãƒ³ãƒ„'])
            complete_record['ã‚¿ã‚°'] = restore_row.get('ã‚¿ã‚°', '')
            complete_record['æœ€çµ‚æ›´æ–°'] = restore_row.get('æœ€çµ‚æ›´æ–°', '')
            complete_record['ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹'] = 'restored'
        else:
            # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åŸ‹ã‚ã‚‹
            complete_record['æ­£ç­”'] = ''
            complete_record['æ­£ç­”ç‡'] = ''
            complete_record['è‹±èªå•é¡Œ'] = 'No'
            complete_record['ç”»åƒå•é¡Œ'] = 'No'
            complete_record['é€£å•'] = 'No'
            complete_record['è¨ˆç®—å•é¡Œ'] = 'No'
            complete_record['å•é¡Œæ–‡'] = ''
            complete_record['ç—‡ä¾‹æ–‡'] = ''
            complete_record['é¸æŠè‚¢'] = ''
            complete_record['Webè¡¨ç¤ºç”¨'] = complete_record['ã‚³ãƒ³ãƒ†ãƒ³ãƒ„']
            complete_record['ã‚¿ã‚°'] = ''
            complete_record['æœ€çµ‚æ›´æ–°'] = ''
            complete_record['ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹'] = 'basic_only'
        
        complete_database[qid] = complete_record
    
    print(f"  å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰å®Œäº†: {len(complete_database)}å•")
    
    # 3. å“è³ªæ¤œè¨¼
    print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—3: å“è³ªæ¤œè¨¼")
    print("-" * 40)
    
    # ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    state_counts = defaultdict(int)
    for record in complete_database.values():
        state_counts[record['ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹']] += 1
    
    print("  ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹åˆ¥å†…è¨³:")
    for state, count in state_counts.items():
        print(f"    {state}: {count}å•")
    
    # å¹´åº¦åˆ¥æ¤œè¨¼
    by_year = defaultdict(list)
    for qid, record in complete_database.items():
        by_year[record['å¹´åº¦']].append(qid)
    
    print("\n  å¹´åº¦åˆ¥æ¤œè¨¼:")
    for year in sorted(by_year.keys()):
        print(f"    ç¬¬{year}å›: {len(by_year[year])}å•")
        
        if len(by_year[year]) != 400:
            print(f"      âš ï¸  äºˆæƒ³ã¨ç•°ãªã‚‹å•é¡Œæ•°")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æ¤œè¨¼
    by_section = defaultdict(list)
    for qid, record in complete_database.items():
        by_section[record['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']].append(qid)
    
    print("\n  ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æ¤œè¨¼:")
    for section in sorted(by_section.keys()):
        print(f"    ã‚»ã‚¯ã‚·ãƒ§ãƒ³{section}: {len(by_section[section])}å•")
    
    # 4. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    print("\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—4: æœ€çµ‚å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
    print("-" * 40)
    
    # CSVå½¢å¼ï¼ˆå®Œå…¨ç‰ˆï¼‰
    final_csv = final_dir / "complete_medical_exam_database_2000.csv"
    
    fieldnames = ['å•é¡ŒID', 'å¹´åº¦', 'ã‚»ã‚¯ã‚·ãƒ§ãƒ³', 'å•é¡Œç•ªå·', 'æ­£ç­”', 'æ­£ç­”ç‡',
                  'è‹±èªå•é¡Œ', 'ç”»åƒå•é¡Œ', 'é€£å•', 'è¨ˆç®—å•é¡Œ', 'å•é¡Œæ–‡', 'ç—‡ä¾‹æ–‡',
                  'é¸æŠè‚¢', 'Webè¡¨ç¤ºç”¨', 'ã‚¿ã‚°', 'æœ€çµ‚æ›´æ–°', 'ã‚½ãƒ¼ã‚¹', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹']
    
    with open(final_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for qid in sorted(complete_database.keys()):
            writer.writerow(complete_database[qid])
    
    print(f"  å®Œå…¨ç‰ˆCSV: {final_csv}")
    
    # JSONå½¢å¼ï¼ˆæ§‹é€ åŒ–ï¼‰
    final_json = final_dir / "complete_medical_exam_database_2000.json"
    
    json_data = {
        'metadata': {
            'total_questions': len(complete_database),
            'creation_date': datetime.now().isoformat(),
            'version': '1.0',
            'description': 'æ—¥æœ¬åŒ»å¸«å›½å®¶è©¦é¨“å•é¡Œå®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆç¬¬115-119å›ï¼‰'
        },
        'statistics': {
            'by_year': {year: len(questions) for year, questions in by_year.items()},
            'by_section': {section: len(questions) for section, questions in by_section.items()},
            'by_data_state': dict(state_counts)
        },
        'questions': complete_database
    }
    
    with open(final_json, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"  JSONå½¢å¼: {final_json}")
    
    # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
    stats_file = final_dir / "database_statistics.md"
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("# å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"## åŸºæœ¬æƒ…å ±\n")
        f.write(f"- ç·å•é¡Œæ•°: {len(complete_database)}å•\n")
        f.write(f"- å¯¾è±¡å¹´åº¦: ç¬¬115-119å›åŒ»å¸«å›½å®¶è©¦é¨“\n")
        f.write(f"- ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## å¹´åº¦åˆ¥å†…è¨³\n")
        for year in sorted(by_year.keys()):
            f.write(f"- ç¬¬{year}å›: {len(by_year[year])}å•\n")
        
        f.write("\n## ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å†…è¨³\n")
        for section in sorted(by_section.keys()):
            f.write(f"- ã‚»ã‚¯ã‚·ãƒ§ãƒ³{section}: {len(by_section[section])}å•\n")
        
        f.write("\n## ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹åˆ¥å†…è¨³\n")
        for state, count in state_counts.items():
            f.write(f"- {state}: {count}å•\n")
        
        f.write("\n## å¾©å…ƒæˆåŠŸç‡\n")
        total = len(complete_database)
        complete_data = state_counts['notion_existing'] + state_counts['restored']
        f.write(f"å®Œå…¨ãƒ‡ãƒ¼ã‚¿: {complete_data}/{total}å• ({complete_data/total*100:.1f}%)\n")
    
    print(f"  çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ: {stats_file}")
    
    # 5. æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—5: æœ€çµ‚æ¤œè¨¼")
    print("-" * 40)
    
    total_questions = len(complete_database)
    complete_data_count = state_counts['notion_existing'] + state_counts['restored']
    
    print(f"  âœ… ç·å•é¡Œæ•°: {total_questions}å•")
    print(f"  ğŸ“Š å®Œå…¨ãƒ‡ãƒ¼ã‚¿: {complete_data_count}å• ({complete_data_count/total_questions*100:.1f}%)")
    print(f"  ğŸ”„ å¾©å…ƒãƒ‡ãƒ¼ã‚¿: {state_counts['restored']}å•")
    print(f"  ğŸ’¾ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {state_counts['notion_existing']}å•")
    
    if total_questions == 2000:
        print("  ğŸ¯ ç›®æ¨™é”æˆ: 2000å•ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰æˆåŠŸï¼")
    
    # 6. æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print("\nğŸ‰ ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†")
    print("-" * 40)
    
    print("  ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"     - {final_csv.name} (CSVå®Œå…¨ç‰ˆ)")
    print(f"     - {final_json.name} (JSONæ§‹é€ åŒ–ç‰ˆ)")
    print(f"     - {stats_file.name} (çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ)")
    
    print("\n  ğŸš€ JMLEèª¬æ˜æ–‡ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†")
    print("     - 2000å•ã®åŒ»å¸«å›½å®¶è©¦é¨“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")
    print("     - å®Œå…¨ãªå•é¡Œæ§‹é€ ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
    print("     - èª¬æ˜æ–‡ç”Ÿæˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸºç›¤")
    
    return {
        'total_questions': total_questions,
        'complete_data_count': complete_data_count,
        'success_rate': complete_data_count / total_questions,
        'output_files': [final_csv, final_json, stats_file]
    }

if __name__ == "__main__":
    build_complete_database()