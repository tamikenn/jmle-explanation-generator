#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆã®è©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import re
import csv
from pathlib import Path
from collections import defaultdict

def extract_question_ids_from_text(file_path):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡ŒIDã‚’æŠ½å‡º"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    pattern = r'\b(\d{3}[A-Z]\d{1,3})\b'
    ids = sorted(set(re.findall(pattern, content)))
    return ids

def extract_question_ids_from_csv(file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡ŒIDã‚’æŠ½å‡º"""
    ids = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # BOMã‚ã‚Šã®å•é¡ŒIDã‚«ãƒ©ãƒ ã‚’æ¢ã™
            qid = None
            for key in row.keys():
                if 'å•é¡ŒID' in key:
                    qid = row[key]
                    break
            
            if qid and qid.strip() and qid != 'å•é¡ŒID':  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤å¤–
                # å•é¡ŒIDã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if re.match(r'\d{3}[A-Z]\d{1,3}', qid.strip()):
                    ids.append(qid.strip())
    return sorted(set(ids))

def analyze_data_discrepancies():
    """ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆã®è©³ç´°åˆ†æ"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆè©³ç´°åˆ†æ")
    print("=" * 60)
    
    # ãƒ‘ã‚¹è¨­å®š
    source_dir = Path("/workspaces/jmle-explanation-generator/raw_data/source_texts")
    notion_dir = Path("/workspaces/jmle-explanation-generator/raw_data/notion")
    
    # 1. å…ƒãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡ŒIDæŠ½å‡º
    source_files = sorted(source_dir.glob("medical_exam_11*.txt"))
    source_files = [f for f in source_files if "web_display" not in f.name]
    
    all_source_ids = []
    source_by_year = defaultdict(list)
    
    print("\nğŸ“ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ:")
    for file in source_files:
        ids = extract_question_ids_from_text(file)
        all_source_ids.extend(ids)
        
        # å¹´åº¦åˆ¥ã«åˆ†é¡
        for qid in ids:
            year = qid[:3]
            source_by_year[year].append(qid)
        
        print(f"  {file.name}: {len(ids)}å•")
    
    all_source_ids = sorted(set(all_source_ids))
    
    # 2. å®Œæˆå½¢ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡ŒIDæŠ½å‡º
    web_file = source_dir / "medical_exam_web_display_final.txt"
    web_ids = []
    if web_file.exists():
        web_ids = extract_question_ids_from_text(web_file)
    
    # 3. NotionCSVã‹ã‚‰å•é¡ŒIDæŠ½å‡º
    notion_files = list(notion_dir.glob("*.csv"))
    notion_ids = []
    if notion_files:
        notion_file = notion_files[0]  # æœ€åˆã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        print(f"  Notionãƒ•ã‚¡ã‚¤ãƒ«: {notion_file.name}")
        notion_ids = extract_question_ids_from_csv(notion_file)
    
    # 4. å·®ç•°åˆ†æ
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ:")
    print(f"  å…ƒãƒ‡ãƒ¼ã‚¿ (source): {len(all_source_ids)}å•")
    print(f"  å®Œæˆå½¢ (web): {len(web_ids)}å•")
    print(f"  Notion: {len(notion_ids)}å•")
    
    # 5. å®Œæˆå½¢ã§å¢—ãˆãŸå•é¡Œã‚’ç‰¹å®š
    extra_in_web = sorted(set(web_ids) - set(all_source_ids))
    missing_in_web = sorted(set(all_source_ids) - set(web_ids))
    
    print(f"\nğŸ” å®Œæˆå½¢ã§ã®å¤‰æ›´:")
    print(f"  è¿½åŠ ã•ã‚ŒãŸå•é¡Œ: {len(extra_in_web)}å•")
    if extra_in_web[:10]:
        print(f"  è¿½åŠ ä¾‹: {extra_in_web[:10]}")
    
    print(f"  æ¬ è½ã—ãŸå•é¡Œ: {len(missing_in_web)}å•")
    if missing_in_web[:10]:
        print(f"  æ¬ è½ä¾‹: {missing_in_web[:10]}")
    
    # 6. Notionã§ã®æ¬ æã‚’ç‰¹å®š
    missing_in_notion = sorted(set(web_ids) - set(notion_ids))
    extra_in_notion = sorted(set(notion_ids) - set(web_ids))
    
    print(f"\nğŸ” Notionã§ã®å¤‰æ›´:")
    print(f"  æ¬ æå•é¡Œ: {len(missing_in_notion)}å•")
    if missing_in_notion[:10]:
        print(f"  æ¬ æä¾‹: {missing_in_notion[:10]}")
    
    print(f"  ä½™åˆ†ãªå•é¡Œ: {len(extra_in_notion)}å•")
    if extra_in_notion[:10]:
        print(f"  ä½™åˆ†ä¾‹: {extra_in_notion[:10]}")
    
    # 7. å¹´åº¦åˆ¥åˆ†æ
    print(f"\nğŸ“… å¹´åº¦åˆ¥åˆ†æ:")
    years = ['115', '116', '117', '118', '119']
    
    for year in years:
        source_count = len([qid for qid in all_source_ids if qid.startswith(year)])
        web_count = len([qid for qid in web_ids if qid.startswith(year)])
        notion_count = len([qid for qid in notion_ids if qid.startswith(year)])
        
        print(f"  ç¬¬{year}å›:")
        print(f"    å…ƒãƒ‡ãƒ¼ã‚¿: {source_count}å•")
        print(f"    å®Œæˆå½¢: {web_count}å•")
        print(f"    Notion: {notion_count}å•")
        
        if web_count != source_count:
            year_extra = [qid for qid in extra_in_web if qid.startswith(year)]
            year_missing = [qid for qid in missing_in_web if qid.startswith(year)]
            if year_extra:
                print(f"    è¿½åŠ : {len(year_extra)}å• {year_extra[:5]}")
            if year_missing:
                print(f"    æ¬ è½: {len(year_missing)}å• {year_missing[:5]}")
    
    # 8. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    print(f"\nğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
    print(f"  1. å®Œæˆå½¢ãƒ‡ãƒ¼ã‚¿ (2,000å•) ã‚’åŸºæº–ã¨ã™ã‚‹")
    print(f"  2. Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¬ æã—ã¦ã„ã‚‹ {len(missing_in_notion)}å•ã‚’è£œå®Œ")
    print(f"  3. å…ƒãƒ‡ãƒ¼ã‚¿ã¨å®Œæˆå½¢ã®å·®ç•° {len(extra_in_web)}å•ã®è¿½åŠ ç†ç”±ã‚’ç¢ºèª")
    
    return {
        'source_ids': all_source_ids,
        'web_ids': web_ids,
        'notion_ids': notion_ids,
        'missing_in_notion': missing_in_notion,
        'extra_in_web': extra_in_web
    }

if __name__ == "__main__":
    analyze_data_discrepancies()