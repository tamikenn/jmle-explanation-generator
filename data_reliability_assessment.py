#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§è©•ä¾¡ã¨å„ªå…ˆé †ä½æ±ºå®š
"""
import re
from pathlib import Path
from collections import defaultdict

def assess_data_reliability():
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä¿¡é ¼æ€§è©•ä¾¡")
    print("=" * 60)
    
    source_dir = Path("/workspaces/jmle-explanation-generator/raw_data/source_texts")
    
    # 1. medical_exam_11~ã‚·ãƒªãƒ¼ã‚ºï¼ˆæœ€ã‚‚ä¿¡é ¼ã§ãã‚‹å…ƒãƒ‡ãƒ¼ã‚¿ï¼‰
    print("\nğŸ“Š 1. medical_exam_11~ã‚·ãƒªãƒ¼ã‚ºï¼ˆæœ€ã‚‚ä¿¡é ¼ã§ãã‚‹ï¼‰")
    print("-" * 40)
    
    source_files = sorted([f for f in source_dir.glob("medical_exam_11*.txt") 
                          if "web_display" not in f.name])
    
    total_source = 0
    source_by_year = defaultdict(list)
    
    for file in source_files:
        with open(file, 'r', encoding='utf-8') as f:
            content = f.read()
            pattern = r'\b(\d{3}[A-Z]\d{1,3})\b'
            ids = sorted(set(re.findall(pattern, content)))
            
            print(f"  {file.name}: {len(ids)}å•")
            total_source += len(ids)
            
            # å¹´åº¦åˆ¥åˆ†é¡
            for qid in ids:
                year = qid[:3]
                source_by_year[year].append(qid)
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            lines = content.split('\n')
            print(f"    ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file.stat().st_size:,}ãƒã‚¤ãƒˆ")
            print(f"    è¡Œæ•°: {len(lines):,}")
            
            # å•é¡Œæ§‹é€ ã®ç¢ºèª
            sample_problems = 0
            for line in lines[:100]:
                if re.match(r'\d{3}[A-Z]\d{1,3}\s', line):
                    sample_problems += 1
            print(f"    å•é¡Œæ§‹é€ ã®æ•´åˆæ€§: {sample_problems}/100è¡Œã§ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª")
    
    print(f"\n  ğŸ¯ åˆè¨ˆ: {total_source}å•")
    
    # 2. web_displayå®Œæˆå½¢ãƒ‡ãƒ¼ã‚¿
    print("\nğŸ“Š 2. medical_exam_web_display_final.txtï¼ˆå®Œæˆå½¢ï¼‰")
    print("-" * 40)
    
    web_file = source_dir / "medical_exam_web_display_final.txt"
    web_ids = []
    if web_file.exists():
        with open(web_file, 'r', encoding='utf-8') as f:
            content = f.read()
            pattern = r'\b(\d{3}[A-Z]\d{1,3})\b'
            web_ids = sorted(set(re.findall(pattern, content)))
            
            lines = content.split('\n')
            print(f"  å•é¡Œæ•°: {len(web_ids)}å•")
            print(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {web_file.stat().st_size:,}ãƒã‚¤ãƒˆ")
            print(f"  è¡Œæ•°: {len(lines):,}")
            
            # å®Œæˆåº¦ãƒã‚§ãƒƒã‚¯
            if len(lines) > 10:
                print(f"  å®Œæˆåº¦æŒ‡æ¨™: ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚ã‚Š")
                for i, line in enumerate(lines[:10]):
                    if "ç·å•é¡Œæ•°" in line or "è‹±èªå•é¡Œ" in line:
                        print(f"    {line.strip()}")
    
    # 3. Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    print("\nğŸ“Š 3. Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆç¾åœ¨ã®çŠ¶æ³ï¼‰")
    print("-" * 40)
    
    notion_dir = Path("/workspaces/jmle-explanation-generator/raw_data/notion")
    notion_files = list(notion_dir.glob("*.csv"))
    
    if notion_files:
        # å‰å›ã®åˆ†æçµæœã‚’è¦ç´„
        print(f"  å•é¡Œæ•°: 1,750å•ï¼ˆå‰å›åˆ†æçµæœï¼‰")
        print(f"  ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§: æ¬ ç•ªãªã—ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯å®Œå…¨ï¼‰")
        print(f"  æ§‹é€ åŒ–ãƒ¬ãƒ™ãƒ«: é«˜ï¼ˆCSVå½¢å¼ã€å®Œå…¨ãªæ§‹é€ åŒ–ï¼‰")
    
    # 4. ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼åˆ†æ
    print("\nğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ•ãƒ­ãƒ¼åˆ†æ")
    print("-" * 40)
    
    all_source_ids = []
    for year_ids in source_by_year.values():
        all_source_ids.extend(year_ids)
    all_source_ids = sorted(set(all_source_ids))
    
    print(f"  å…ƒãƒ‡ãƒ¼ã‚¿ â†’ å®Œæˆå½¢: {len(all_source_ids)} â†’ {len(web_ids)} (+{len(web_ids) - len(all_source_ids)}å•)")
    print(f"  å®Œæˆå½¢ â†’ Notion: {len(web_ids)} â†’ 1750 (-{len(web_ids) - 1750}å•)")
    
    # 5. æ¨å¥¨æˆ¦ç•¥
    print("\nğŸ’¡ æ¨å¥¨ãƒ‡ãƒ¼ã‚¿å¾©å…ƒæˆ¦ç•¥")
    print("-" * 40)
    
    print("  ã€ãƒ•ã‚§ãƒ¼ã‚º1ã€‘ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ç¢ºç«‹")
    print("    - medical_exam_11~ã‚·ãƒªãƒ¼ã‚ºï¼ˆ1,850å•ï¼‰ã‚’ä¿¡é ¼ã§ãã‚‹ãƒ™ãƒ¼ã‚¹ã¨ã™ã‚‹")
    print("    - web_displayã§è¿½åŠ ã•ã‚ŒãŸ150å•ã®å“è³ªè©•ä¾¡")
    
    print("\n  ã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘ ãƒ‡ãƒ¼ã‚¿çµ±åˆ")
    print("    - ãƒ™ãƒ¼ã‚¹1,850å• + å“è³ªç¢ºèªæ¸ˆã¿è¿½åŠ 150å• = 2,000å•")
    print("    - ç¾åœ¨ã®Notion1,750å•ã¨ã®å·®åˆ†251å•ã‚’ç‰¹å®š")
    
    print("\n  ã€ãƒ•ã‚§ãƒ¼ã‚º3ã€‘ å®Œå…¨å¾©å…ƒ")
    print("    - æ¬ æ251å•ã‚’web_displayã‹ã‚‰æŠ½å‡º")
    print("    - æœ€çµ‚çš„ã«2,000å•ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰")
    
    return {
        'source_ids': all_source_ids,
        'web_ids': web_ids,
        'source_count': len(all_source_ids),
        'web_count': len(web_ids)
    }

if __name__ == "__main__":
    assess_data_reliability()