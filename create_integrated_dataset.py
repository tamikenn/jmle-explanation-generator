#!/usr/bin/env python3
"""
ãƒ•ã‚§ãƒ¼ã‚º2: 1850å• + 150å• = 2000å•ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
"""
import re
import csv
from pathlib import Path
from collections import defaultdict, OrderedDict

def create_integrated_dataset():
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    print("ğŸ”§ çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆãƒ•ã‚§ãƒ¼ã‚º2ï¼‰")
    print("=" * 60)
    
    source_dir = Path("/workspaces/jmle-explanation-generator/raw_data/source_texts")
    output_dir = Path("/workspaces/jmle-explanation-generator/raw_data/exports")
    output_dir.mkdir(exist_ok=True)
    
    # 1. ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆ1850å•ï¼‰ã®æŠ½å‡º
    print("\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆ1850å•ï¼‰æŠ½å‡º")
    print("-" * 40)
    
    base_questions = {}
    source_files = sorted([f for f in source_dir.glob("medical_exam_11*.txt") 
                          if "web_display" not in f.name])
    
    for file in source_files:
        print(f"  å‡¦ç†ä¸­: {file.name}")
        with open(file, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # å•é¡ŒIDãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ†å‰²
            pattern = r'(\d{3}[A-Z]\d{1,3})'
            parts = re.split(pattern, content)
            
            for i in range(1, len(parts), 2):
                if i + 1 < len(parts):
                    qid = parts[i]
                    text = parts[i + 1]
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    clean_text = '\n'.join(lines)
                    
                    if clean_text and re.match(r'\d{3}[A-Z]\d{1,3}', qid):
                        base_questions[qid] = {
                            'id': qid,
                            'source': 'base_data',
                            'content': clean_text[:1000],  # é•·ã™ãã‚‹å ´åˆã¯åˆ¶é™
                            'year': qid[:3],
                            'section': qid[3],
                            'number': qid[4:]
                        }
    
    print(f"  ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(base_questions)}å•")
    
    # 2. è¿½åŠ ãƒ‡ãƒ¼ã‚¿ï¼ˆ150å•ï¼‰ã®æŠ½å‡º
    print("\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ—2: è¿½åŠ ãƒ‡ãƒ¼ã‚¿ï¼ˆ150å•ï¼‰æŠ½å‡º")
    print("-" * 40)
    
    web_file = source_dir / "medical_exam_web_display_final.txt"
    added_questions = {}
    
    if web_file.exists():
        with open(web_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # å…¨å•é¡ŒIDã‚’æŠ½å‡º
            pattern = r'\b(\d{3}[A-Z]\d{1,3})\b'
            all_web_ids = sorted(set(re.findall(pattern, content)))
            
            # è¿½åŠ å•é¡Œã®ã¿ã‚’ç‰¹å®š
            added_ids = sorted(set(all_web_ids) - set(base_questions.keys()))
            
            print(f"  è¿½åŠ å•é¡Œç‰¹å®š: {len(added_ids)}å•")
            
            # è¿½åŠ å•é¡Œã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
            for qid in added_ids:
                # è©²å½“å•é¡Œã®å†…å®¹ã‚’æŠ½å‡º
                qid_pattern = fr'{re.escape(qid)}.*?(?=\d{{3}}[A-Z]\d{{1,3}}|$)'
                match = re.search(qid_pattern, content, re.DOTALL)
                
                if match:
                    text = match.group(0)
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    clean_text = '\n'.join(lines)
                    
                    added_questions[qid] = {
                        'id': qid,
                        'source': 'web_display_added',
                        'content': clean_text[:1000],
                        'year': qid[:3],
                        'section': qid[3],
                        'number': qid[4:]
                    }
    
    print(f"  è¿½åŠ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(added_questions)}å•")
    
    # 3. ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    print("\nğŸ”— ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿çµ±åˆ")
    print("-" * 40)
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    integrated_dataset = {}
    integrated_dataset.update(base_questions)
    integrated_dataset.update(added_questions)
    
    print(f"  çµ±åˆå®Œäº†:")
    print(f"    ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {len(base_questions)}å•")
    print(f"    è¿½åŠ ãƒ‡ãƒ¼ã‚¿: {len(added_questions)}å•")
    print(f"    çµ±åˆåˆè¨ˆ: {len(integrated_dataset)}å•")
    
    # 4. å“è³ªãƒã‚§ãƒƒã‚¯
    print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—4: å“è³ªãƒã‚§ãƒƒã‚¯")
    print("-" * 40)
    
    # å¹´åº¦åˆ¥ç¢ºèª
    by_year = defaultdict(list)
    for qid, data in integrated_dataset.items():
        by_year[data['year']].append(qid)
    
    for year in sorted(by_year.keys()):
        year_count = len(by_year[year])
        base_count = len([q for q in by_year[year] if q in base_questions])
        added_count = len([q for q in by_year[year] if q in added_questions])
        
        print(f"  ç¬¬{year}å›: {year_count}å• (ãƒ™ãƒ¼ã‚¹:{base_count} + è¿½åŠ :{added_count})")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    all_ids = list(integrated_dataset.keys())
    unique_ids = set(all_ids)
    if len(all_ids) != len(unique_ids):
        print(f"  âš ï¸  é‡è¤‡ã‚ã‚Š: {len(all_ids) - len(unique_ids)}å•")
    else:
        print(f"  âœ… é‡è¤‡ãªã—")
    
    # æ¬ ç•ªãƒã‚§ãƒƒã‚¯
    missing_sequences = []
    for year in ['115', '116', '117', '118', '119']:
        year_ids = [qid for qid in all_ids if qid.startswith(year)]
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ãƒã‚§ãƒƒã‚¯
        for section in ['A', 'B', 'C', 'D', 'E', 'F']:
            section_ids = [qid for qid in year_ids if qid[3] == section]
            if section_ids:
                numbers = sorted([int(re.search(r'(\d+)$', qid).group(1)) for qid in section_ids])
                for i in range(min(numbers), max(numbers) + 1):
                    expected_id = f"{year}{section}{i}"
                    if expected_id not in section_ids:
                        missing_sequences.append(expected_id)
    
    if missing_sequences:
        print(f"  âš ï¸  æ¬ ç•ªæ¤œå‡º: {len(missing_sequences)}å•")
        print(f"     ä¾‹: {missing_sequences[:10]}")
    else:
        print(f"  âœ… æ¬ ç•ªãªã—")
    
    # 5. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    print("\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—5: çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡ºåŠ›")
    print("-" * 40)
    
    # CSVå½¢å¼ã§å‡ºåŠ›
    output_csv = output_dir / "integrated_dataset_2000.csv"
    
    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['å•é¡ŒID', 'å¹´åº¦', 'ã‚»ã‚¯ã‚·ãƒ§ãƒ³', 'å•é¡Œç•ªå·', 'ã‚½ãƒ¼ã‚¹', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for qid in sorted(integrated_dataset.keys()):
            data = integrated_dataset[qid]
            writer.writerow({
                'å•é¡ŒID': data['id'],
                'å¹´åº¦': data['year'],
                'ã‚»ã‚¯ã‚·ãƒ§ãƒ³': data['section'],
                'å•é¡Œç•ªå·': data['number'],
                'ã‚½ãƒ¼ã‚¹': data['source'],
                'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„': data['content']
            })
    
    print(f"  CSVå‡ºåŠ›å®Œäº†: {output_csv}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    summary_file = output_dir / "integration_summary.txt"
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("# çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚µãƒãƒªãƒ¼\n\n")
        f.write(f"ä½œæˆæ—¥æ™‚: {Path().cwd()}\n")
        f.write(f"ç·å•é¡Œæ•°: {len(integrated_dataset)}å•\n\n")
        f.write("## å¹´åº¦åˆ¥å†…è¨³\n")
        
        for year in sorted(by_year.keys()):
            year_count = len(by_year[year])
            base_count = len([q for q in by_year[year] if q in base_questions])
            added_count = len([q for q in by_year[year] if q in added_questions])
            f.write(f"ç¬¬{year}å›: {year_count}å• (ãƒ™ãƒ¼ã‚¹:{base_count} + è¿½åŠ :{added_count})\n")
        
        f.write(f"\n## ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹\n")
        f.write(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {len(base_questions)}å• (medical_exam_11~ã‚·ãƒªãƒ¼ã‚º)\n")
        f.write(f"è¿½åŠ ãƒ‡ãƒ¼ã‚¿: {len(added_questions)}å• (web_display_final)\n")
        
        if missing_sequences:
            f.write(f"\n## æ¬ ç•ª\n")
            for missing in missing_sequences[:20]:
                f.write(f"{missing}\n")
    
    print(f"  ã‚µãƒãƒªãƒ¼å‡ºåŠ›å®Œäº†: {summary_file}")
    
    # 6. æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºã¸ã®æº–å‚™
    print("\nğŸš€ ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ•ã‚§ãƒ¼ã‚º3æº–å‚™")
    print("-" * 40)
    
    print("  âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†")
    print("  ğŸ“‹ æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚º: Notionæ¬ æ251å•ã®ç‰¹å®šã¨å¾©å…ƒ")
    print("  ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"     - {output_csv.name} (CSVå½¢å¼)")
    print(f"     - {summary_file.name} (ã‚µãƒãƒªãƒ¼)")
    
    return {
        'total_questions': len(integrated_dataset),
        'base_questions': len(base_questions),
        'added_questions': len(added_questions),
        'output_csv': output_csv,
        'missing_sequences': missing_sequences
    }

if __name__ == "__main__":
    create_integrated_dataset()